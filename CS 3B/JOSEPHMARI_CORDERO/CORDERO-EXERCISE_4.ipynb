{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1703373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: X_train=(800, 10), Y_train=(800, 3)\n",
      "Validation data shape: X_val=(200, 10), Y_val=(200, 3)\n",
      "Data arrays created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create sample training and validation data for the optimizer comparison\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simple synthetic classification dataset without sklearn\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 1000 samples with 10 features\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "n_classes = 3\n",
    "\n",
    "# Generate random features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create class labels based on simple rules\n",
    "y = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    if X[i, 0] + X[i, 1] > 0.5:\n",
    "        y[i] = 0  # Class 0\n",
    "    elif X[i, 2] - X[i, 3] > 0:\n",
    "        y[i] = 1  # Class 1\n",
    "    else:\n",
    "        y[i] = 2  # Class 2\n",
    "\n",
    "# Split into train (800) and validation (200) sets\n",
    "train_size = 800\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "# Standardize the features manually\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train_std = np.std(X_train, axis=0) + 1e-8  # Add small value to avoid division by zero\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_val = (X_val - X_train_mean) / X_train_std\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def to_one_hot(y, num_classes):\n",
    "    one_hot = np.zeros((len(y), num_classes))\n",
    "    one_hot[np.arange(len(y)), y.astype(int)] = 1\n",
    "    return one_hot\n",
    "\n",
    "Y_train = to_one_hot(y_train, n_classes)\n",
    "Y_val = to_one_hot(y_val, n_classes)\n",
    "\n",
    "print(f\"Training data shape: X_train={X_train.shape}, Y_train={Y_train.shape}\")\n",
    "print(f\"Validation data shape: X_val={X_val.shape}, Y_val={Y_val.shape}\")\n",
    "print(\"Data arrays created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6350871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network Model for the optimizer comparison\n",
    "class SimpleMLPModel:\n",
    "    def __init__(self, in_dim=10, hidden_dim=64, out_dim=3):\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(in_dim, hidden_dim) * 0.1,\n",
    "            'b1': np.zeros((1, hidden_dim)),\n",
    "            'W2': np.random.randn(hidden_dim, out_dim) * 0.1,\n",
    "            'b2': np.zeros((1, out_dim))\n",
    "        }\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # First layer\n",
    "        z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        # Second layer (output)\n",
    "        z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "        a2 = self.softmax(z2)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        cache = {\n",
    "            'X': X,\n",
    "            'z1': z1,\n",
    "            'a1': a1,\n",
    "            'z2': z2,\n",
    "            'a2': a2\n",
    "        }\n",
    "        \n",
    "        return a2, cache\n",
    "    \n",
    "    def compute_loss(self, probs, Y_true):\n",
    "        # Cross-entropy loss\n",
    "        m = Y_true.shape[0]\n",
    "        log_likelihood = -np.log(probs + 1e-15)\n",
    "        loss = np.sum(log_likelihood * Y_true) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, cache, Y_true):\n",
    "        m = Y_true.shape[0]\n",
    "        \n",
    "        # Extract cached values\n",
    "        X = cache['X']\n",
    "        a1 = cache['a1']\n",
    "        a2 = cache['a2']\n",
    "        z1 = cache['z1']\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = a2 - Y_true\n",
    "        dW2 = np.dot(a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.params['W2'].T)\n",
    "        dz1 = da1 * (z1 > 0)  # ReLU derivative\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        grads = {\n",
    "            'W1': dW1,\n",
    "            'b1': db1,\n",
    "            'W2': dW2,\n",
    "            'b2': db2\n",
    "        }\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs, _ = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# Create the model instance\n",
    "model = SimpleMLPModel(in_dim=X_train.shape[1], hidden_dim=64, out_dim=3)\n",
    "print(\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec51e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting experiment: momentum\n",
      "[momentum] Epoch 1/1000 | lr=0.050000 | loss=1.081990 | train_acc=0.4150 | val_acc=0.3800\n",
      "[momentum] Epoch 100/1000 | lr=0.049510 | loss=0.108952 | train_acc=0.9775 | val_acc=0.9600\n",
      "[momentum] Epoch 200/1000 | lr=0.049024 | loss=0.061799 | train_acc=0.9950 | val_acc=0.9750\n",
      "[momentum] Epoch 300/1000 | lr=0.048548 | loss=0.042032 | train_acc=1.0000 | val_acc=0.9750\n",
      "[momentum] Epoch 200/1000 | lr=0.049024 | loss=0.061799 | train_acc=0.9950 | val_acc=0.9750\n",
      "[momentum] Epoch 300/1000 | lr=0.048548 | loss=0.042032 | train_acc=1.0000 | val_acc=0.9750\n",
      "[momentum] Epoch 400/1000 | lr=0.048082 | loss=0.031271 | train_acc=1.0000 | val_acc=0.9850\n",
      "[momentum] Epoch 500/1000 | lr=0.047624 | loss=0.024449 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 600/1000 | lr=0.047174 | loss=0.019783 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 400/1000 | lr=0.048082 | loss=0.031271 | train_acc=1.0000 | val_acc=0.9850\n",
      "[momentum] Epoch 500/1000 | lr=0.047624 | loss=0.024449 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 600/1000 | lr=0.047174 | loss=0.019783 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 700/1000 | lr=0.046733 | loss=0.016476 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 800/1000 | lr=0.046301 | loss=0.014009 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 700/1000 | lr=0.046733 | loss=0.016476 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 800/1000 | lr=0.046301 | loss=0.014009 | train_acc=1.0000 | val_acc=0.9900\n",
      "[momentum] Epoch 900/1000 | lr=0.045876 | loss=0.012116 | train_acc=1.0000 | val_acc=0.9850\n",
      "[momentum] Epoch 1000/1000 | lr=0.045459 | loss=0.010636 | train_acc=1.0000 | val_acc=0.9850\n",
      "\n",
      "============================================================\n",
      "Starting experiment: adagrad\n",
      "[adagrad] Epoch 1/1000 | lr=0.500000 | loss=1.081990 | train_acc=0.7200 | val_acc=0.6900\n",
      "[adagrad] Epoch 100/1000 | lr=0.495099 | loss=0.007424 | train_acc=1.0000 | val_acc=0.9500\n",
      "[momentum] Epoch 900/1000 | lr=0.045876 | loss=0.012116 | train_acc=1.0000 | val_acc=0.9850\n",
      "[momentum] Epoch 1000/1000 | lr=0.045459 | loss=0.010636 | train_acc=1.0000 | val_acc=0.9850\n",
      "\n",
      "============================================================\n",
      "Starting experiment: adagrad\n",
      "[adagrad] Epoch 1/1000 | lr=0.500000 | loss=1.081990 | train_acc=0.7200 | val_acc=0.6900\n",
      "[adagrad] Epoch 100/1000 | lr=0.495099 | loss=0.007424 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 200/1000 | lr=0.490244 | loss=0.003950 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 300/1000 | lr=0.485484 | loss=0.002592 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 400/1000 | lr=0.480815 | loss=0.001901 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 200/1000 | lr=0.490244 | loss=0.003950 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 300/1000 | lr=0.485484 | loss=0.002592 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 400/1000 | lr=0.480815 | loss=0.001901 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 500/1000 | lr=0.476236 | loss=0.001488 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 600/1000 | lr=0.471743 | loss=0.001208 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 700/1000 | lr=0.467333 | loss=0.001009 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 500/1000 | lr=0.476236 | loss=0.001488 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 600/1000 | lr=0.471743 | loss=0.001208 | train_acc=1.0000 | val_acc=0.9450\n",
      "[adagrad] Epoch 700/1000 | lr=0.467333 | loss=0.001009 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 800/1000 | lr=0.463006 | loss=0.000873 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 900/1000 | lr=0.458758 | loss=0.000771 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 800/1000 | lr=0.463006 | loss=0.000873 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 900/1000 | lr=0.458758 | loss=0.000771 | train_acc=1.0000 | val_acc=0.9500\n",
      "[adagrad] Epoch 1000/1000 | lr=0.454587 | loss=0.000690 | train_acc=1.0000 | val_acc=0.9550\n",
      "\n",
      "============================================================\n",
      "Comparison summary (stabilize epoch = start of the last stable window):\n",
      "Optimizer    | StabilizeEpoch   | FinalValAcc \n",
      "momentum     | N/A              | 0.9850\n",
      "adagrad      | N/A              | 0.9550\n",
      "[adagrad] Epoch 1000/1000 | lr=0.454587 | loss=0.000690 | train_acc=1.0000 | val_acc=0.9550\n",
      "\n",
      "============================================================\n",
      "Comparison summary (stabilize epoch = start of the last stable window):\n",
      "Optimizer    | StabilizeEpoch   | FinalValAcc \n",
      "momentum     | N/A              | 0.9850\n",
      "adagrad      | N/A              | 0.9550\n"
     ]
    }
   ],
   "source": [
    "# ---------- Appended cell: Optimizers + 1000-epoch training & comparison ----------\n",
    "# Paste this cell at the end of your backpropagation.ipynb (after your network, forward/backward, and data).\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- Unified Optimizer -----------------\n",
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Unified Optimizer supporting 'sgd', 'momentum', and 'adagrad'.\n",
    "    lr_decay is applied with formula: lr = lr0 / (1 + lr_decay * epoch)\n",
    "    NOTE: According to the hint, lr decay is computed BEFORE FP/BP in each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, optimizer_type='sgd', lr=0.1, lr_decay=0.0, momentum=0.9, epsilon=1e-8):\n",
    "        self.optimizer_type = optimizer_type.lower()\n",
    "        self.lr0 = lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        # state initialization (match params keys)\n",
    "        self.state_v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.state_ss = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "    def get_lr(self, epoch):\n",
    "        return self.lr0 / (1.0 + self.lr_decay * epoch)\n",
    "\n",
    "    def step(self, params, grads, epoch):\n",
    "        lr = self.get_lr(epoch)\n",
    "        if self.optimizer_type == 'sgd':\n",
    "            for k in params.keys():\n",
    "                params[k] -= lr * grads[k]\n",
    "        elif self.optimizer_type == 'momentum':\n",
    "            for k in params.keys():\n",
    "                self.state_v[k] = self.momentum * self.state_v[k] - lr * grads[k]\n",
    "                params[k] += self.state_v[k]\n",
    "        elif self.optimizer_type == 'adagrad':\n",
    "            for k in params.keys():\n",
    "                self.state_ss[k] += grads[k] * grads[k]\n",
    "                adjusted_lr = lr / (np.sqrt(self.state_ss[k]) + self.epsilon)\n",
    "                params[k] -= adjusted_lr * grads[k]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer_type: \" + str(self.optimizer_type))\n",
    "\n",
    "# ----------------- Utilities -----------------\n",
    "def compute_accuracy(model, X, Y_onehot):\n",
    "    # expects model.predict(X) returning class indices OR returns probs\n",
    "    if hasattr(model, 'predict'):\n",
    "        preds = model.predict(X)\n",
    "    else:\n",
    "        probs, _ = model.forward(X)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "    true = np.argmax(Y_onehot, axis=1)\n",
    "    return np.mean(preds == true)\n",
    "\n",
    "def detect_stabilization(loss_history, window=20, tol=1e-5):\n",
    "    # returns epoch index where stabilization detected (start of last window) or None\n",
    "    if len(loss_history) < window * 2:\n",
    "        return None\n",
    "    last = np.mean(loss_history[-window:])\n",
    "    prev = np.mean(loss_history[-2*window:-window])\n",
    "    if abs(last - prev) < tol:\n",
    "        return len(loss_history) - window\n",
    "    return None\n",
    "\n",
    "# ----------------- Find / prepare model and data in notebook namespace -----------------\n",
    "# This cell expects the notebook to already define:\n",
    "# - training data arrays: X_train, Y_train (one-hot), X_val, Y_val (one-hot)\n",
    "# - a model instance 'model' OR a model class in globals() that can be instantiated\n",
    "if 'X_train' not in globals() or 'Y_train' not in globals() or 'X_val' not in globals() or 'Y_val' not in globals():\n",
    "    raise RuntimeError(\"Training/validation arrays X_train, Y_train, X_val, Y_val must exist in the notebook. Define them and re-run this cell.\")\n",
    "\n",
    "# Try to find a ready model instance\n",
    "_base_model = None\n",
    "if 'model' in globals():\n",
    "    _base_model = model\n",
    "else:\n",
    "    # attempt to instantiate a model class (try common names)\n",
    "    for name, obj in globals().items():\n",
    "        if isinstance(obj, type) and any(key in name.lower() for key in ('mlp','network','net','model')):\n",
    "            try:\n",
    "                # try common constructor signatures (in_dim, hidden_dim, out_dim) - fallback if not available\n",
    "                _base_model = obj(in_dim=X_train.shape[1], hidden_dim=64, out_dim=Y_train.shape[1])\n",
    "                print(f\"Instantiated model from class: {name}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                # try empty constructor\n",
    "                try:\n",
    "                    _base_model = obj()\n",
    "                    print(f\"Instantiated model from class (no-arg): {name}\")\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "if _base_model is None:\n",
    "    raise RuntimeError(\"No model instance found. Either set a `model` variable in the notebook or ensure a Model class with an instantiable constructor exists.\")\n",
    "\n",
    "# Verify model has params dict access. If not, try to build one from W/b attributes.\n",
    "if not hasattr(_base_model, 'params'):\n",
    "    # attempt to collect weights named W1, b1, W2, b2 from model attributes\n",
    "    params = {}\n",
    "    for attr in ['W1','b1','W2','b2','weights','bias']:\n",
    "        if hasattr(_base_model, attr):\n",
    "            params[attr] = getattr(_base_model, attr)\n",
    "    if len(params) == 0:\n",
    "        raise RuntimeError(\"Model does not expose `params` dict or named weight attributes (W1, b1, W2, b2). Edit model to expose params or set model.params = {...}).\")\n",
    "    else:\n",
    "        # expose params as model.params for optimizer compatibility\n",
    "        _base_model.params = params\n",
    "\n",
    "# ----------------- Training function -----------------\n",
    "def train_copy_with_optimizer(base_model, optimizer_name='sgd', lr=0.1, lr_decay=1e-4, momentum=0.9, epochs=1000, print_every=100):\n",
    "    # create a deep copy so original model remains unchanged\n",
    "    model_copy = copy.deepcopy(base_model)\n",
    "    if not hasattr(model_copy, 'params'):\n",
    "        raise RuntimeError(\"Model copy missing params.\")\n",
    "\n",
    "    opt = Optimizer(model_copy.params, optimizer_type=optimizer_name, lr=lr, lr_decay=lr_decay, momentum=momentum)\n",
    "    loss_history = []\n",
    "    stabilize_epoch = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Apply learning rate decay BEFORE forward/backprop as required\n",
    "        current_lr = opt.get_lr(epoch)\n",
    "\n",
    "        # Forward pass (expects forward to return (probs, cache) - adjust if your forward returns only probs)\n",
    "        out = model_copy.forward(X_train)\n",
    "        if isinstance(out, tuple) and len(out) == 2:\n",
    "            probs, cache = out\n",
    "        else:\n",
    "            # if forward returns probs only, no cache available -> can't backprop\n",
    "            raise RuntimeError(\"model.forward must return (probs, cache). Adjust your forward to return cache used by backward().\")\n",
    "\n",
    "        loss = model_copy.compute_loss(probs, Y_train)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Backprop: expects backward(cache, Y_true) -> grads dict matching model.params keys\n",
    "        grads = model_copy.backward(cache, Y_train)\n",
    "\n",
    "        # Weight update (after lr decay)\n",
    "        opt.step(model_copy.params, grads, epoch)\n",
    "\n",
    "        # detect stabilization\n",
    "        if stabilize_epoch is None:\n",
    "            s = detect_stabilization(loss_history, window=20, tol=1e-5)\n",
    "            if s is not None:\n",
    "                stabilize_epoch = s\n",
    "\n",
    "        # print progress\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            acc_train = compute_accuracy(model_copy, X_train, Y_train)\n",
    "            acc_val = compute_accuracy(model_copy, X_val, Y_val)\n",
    "            print(f\"[{optimizer_name}] Epoch {epoch+1}/{epochs} | lr={current_lr:.6f} | loss={loss:.6f} | train_acc={acc_train:.4f} | val_acc={acc_val:.4f}\")\n",
    "\n",
    "    final_val_acc = compute_accuracy(model_copy, X_val, Y_val)\n",
    "    return {\n",
    "        'model': model_copy,\n",
    "        'loss_history': loss_history,\n",
    "        'stabilize_epoch': stabilize_epoch,\n",
    "        'final_val_acc': final_val_acc\n",
    "    }\n",
    "\n",
    "# ----------------- Run experiments: compare two optimizers (you can pick any two) -----------------\n",
    "# In this example we run Momentum vs Adagrad to compare â€” change names if you prefer.\n",
    "experiments_to_run = [\n",
    "    {'name': 'momentum', 'lr': 0.05, 'lr_decay': 1e-4, 'momentum': 0.9},\n",
    "    {'name': 'adagrad',  'lr': 0.5,  'lr_decay': 1e-4, 'momentum': 0.0},\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for exp in experiments_to_run:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Starting experiment: {exp['name']}\")\n",
    "    res = train_copy_with_optimizer(_base_model, optimizer_name=exp['name'], lr=exp['lr'], lr_decay=exp['lr_decay'], momentum=exp['momentum'], epochs=1000, print_every=100)\n",
    "    results[exp['name']] = res\n",
    "\n",
    "# ----------------- Summarize comparison -----------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison summary (stabilize epoch = start of the last stable window):\")\n",
    "print(\"{:12s} | {:16s} | {:12s}\".format(\"Optimizer\",\"StabilizeEpoch\",\"FinalValAcc\"))\n",
    "for k, v in results.items():\n",
    "    se = v['stabilize_epoch'] if v['stabilize_epoch'] is not None else 'N/A'\n",
    "    print(\"{:12s} | {:16s} | {:.4f}\".format(k, str(se), v['final_val_acc']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
